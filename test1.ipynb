{"cells":[{"cell_type":"markdown","metadata":{},"source":["# RAG Demo\n","Using RAG to power up an LLM. We will use Langchain for our example. Langchain framework makes build LLM apps super easy.\n","\n","![./flow.png](./flow.png)"]},{"cell_type":"markdown","metadata":{},"source":["## Install Packages"]},{"cell_type":"code","execution_count":12,"metadata":{"vscode":{"languageId":"shellscript"}},"outputs":[],"source":["! pip3 install -q langchain chromadb pypdf openai"]},{"cell_type":"markdown","metadata":{},"source":["## Steps\n","### Step 1\n","Load a document and extract the contents. For our example, I added a sample PDF from my article in docs folder."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total pages:  4\n","First 100 chars of 2nd page:  dataset. Each model fits for few use cases.Autoenc\n","Metadata:  {'source': 'docs/GenAI-Part1.pdf', 'page': 1}\n"]}],"source":["from langchain.document_loaders import PyPDFLoader\n","loader = PyPDFLoader(\"docs/GenAI-Part1.pdf\")\n","pages = loader.load()\n","\n","# Look into the doc\n","second_page = pages[1]\n","print(\"Total pages: \", len(pages))\n","print(\"First 100 chars of 2nd page: \", second_page.page_content[:50])\n","print(\"Metadata: \", second_page.metadata)"]},{"cell_type":"markdown","metadata":{},"source":["### Step 2\n","Now split the document contents into smaller chunks."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total documents on Charter Splitter:  4\n","Total documents on Recursive Charter Splitter:  6\n","Total documents on Token Splitter:  8\n"]}],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, TokenTextSplitter\n","\n","# Character Splitter\n","c_splitter = CharacterTextSplitter(\n","    chunk_size=1000,\n","    chunk_overlap=150,\n","    separator = '\\n',\n","    length_function = len\n",")\n","\n","c_docs = c_splitter.split_documents(pages)\n","print(\"Total documents on Charter Splitter: \", len(c_docs))\n","\n","# Recursive Character Splitter\n","r_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,\n","    chunk_overlap=150, \n","    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",")\n","\n","r_docs = r_splitter.split_documents(pages)\n","print(\"Total documents on Recursive Charter Splitter: \", len(r_docs))\n","\n","# Token Splitter\n","t_splitter = TokenTextSplitter(\n","    chunk_size=150,\n","    chunk_overlap=10\n",")\n","\n","t_docs = t_splitter.split_documents(pages)\n","print(\"Total documents on Token Splitter: \", len(t_docs))\n"]},{"cell_type":"markdown","metadata":{},"source":["### Step 3\n","Let's take our splits and embed them and then store them into a vector store. We will use [ChromaDB](https://python.langchain.com/docs/integrations/vectorstores/chroma) which is an in-memory DB."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from langchain.vectorstores import Chroma\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","\n","embedding = OpenAIEmbeddings()\n","persist_directory = 'docs/chroma/'"]},{"cell_type":"code","execution_count":16,"metadata":{"vscode":{"languageId":"shellscript"}},"outputs":[],"source":["!rm -rf ./docs/chroma  # remove old database files if any"]},{"cell_type":"markdown","metadata":{},"source":["Let's store the pages of our PDF into Vector Store with Embeddings using OpenAI Embeddings."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total documents on Vector Store:  4\n"]}],"source":["vectordb = Chroma.from_documents(\n","    documents=pages,\n","    embedding=embedding,\n","    persist_directory=persist_directory\n",")\n","\n","print(\"Total documents on Vector Store: \", vectordb._collection.count())"]},{"cell_type":"markdown","metadata":{},"source":["We will do some search and see some relevant content in this vector db."]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Save the DB\n","vectordb.persist()"]},{"cell_type":"markdown","metadata":{},"source":["### Step 4\n","Let's retrieve with different methods"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Similarity Search\n","Page #1  : dataset. Each model fits for few use cases.Autoencoding models — BERT/ROBERTA — Sentiment analysis, \n","Page #2  : The measurement used to understand the training time is 1 “petaflop/s-day”= # floating point operati\n","Page #3  : You can elect to choose a smaller model and then can train it for a specificfield of yours. For exam\n","Page #0  : Generative AI project — Part 1Muthu ArumugamThis article will help you understand how you can get in\n","\n","MMR Search\n","Page #1  : dataset. Each model fits for few use cases.Autoencoding models — BERT/ROBERTA — Sentiment analysis, \n","Page #2  : The measurement used to understand the training time is 1 “petaflop/s-day”= # floating point operati\n"]}],"source":["question = \"What is DDP?\"\n","\n","# Using Similarity Search\n","print(\"\\nSimilarity Search\")\n","docs = vectordb.similarity_search(question, top_k=2)\n","for d in docs:\n","    print(f\"Page #{d.metadata['page']}  : {d.page_content[:100]}\")\n","\n","# Using MMR to diversify the results\n","print(\"\\nMMR Search\")\n","docs = vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)\n","for d in docs:\n","    print(f\"Page #{d.metadata['page']}  : {d.page_content[:100]}\")"]},{"cell_type":"markdown","metadata":{},"source":["We need to do some compression to avoid unnecessary text around the content we are looking for."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Number of requested results 20 is greater than number of elements in index 4, updating n_results = 4\n","/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Document 1:\n","\n","DDP — Distributed Data Parallel\n"]}],"source":["from langchain.llms import OpenAI\n","from langchain.retrievers import ContextualCompressionRetriever\n","from langchain.retrievers.document_compressors import LLMChainExtractor\n","\n","def pretty_print_docs(docs):\n","    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n","\n","# Wrap our vectorstore\n","llm = OpenAI(temperature=0)\n","compressor = LLMChainExtractor.from_llm(llm)\n","\n","compression_retriever = ContextualCompressionRetriever(\n","    base_compressor=compressor,\n","    base_retriever=vectordb.as_retriever(search_type=\"mmr\")\n",")\n","\n","compressed_docs = compression_retriever.get_relevant_documents(question)\n","pretty_print_docs(compressed_docs)"]},{"cell_type":"markdown","metadata":{},"source":["### Step 5\n","Time to call ChatGPT for a response based on our retrieval. We will use Question & Answer to call LLM.\n","\n","#### Using Retrieval QA Chain"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Question: What is DDP?\n","Answer: DDP stands for Distributed Data Parallel. It is a strategy used to train deep learning models in a distributed manner across multiple GPUs or machines. DDP divides the training data into smaller batches and distributes them across the available resources. Each resource (GPU or machine) then independently computes the gradients for its batch and communicates them with the other resources. This allows for parallel processing and faster training of the model. DDP is commonly used in large-scale deep learning projects to optimize the use of computing resources.\n"]}],"source":["from langchain.chat_models import ChatOpenAI\n","from langchain.chains import RetrievalQA\n","\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    retriever=vectordb.as_retriever()\n",")\n","\n","result = qa_chain({\"query\": question})\n","print(f\"Question: {question}\\nAnswer: {result['result']}\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Using Prompt"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Question: What is DDP?\n","Answer: DDP stands for Distributed Data Parallel. It is a strategy for training deep learning models that involves distributing the data across multiple GPUs and parallelizing the computations to speed up the training process. Thanks for asking!\n"]}],"source":["from langchain.prompts import PromptTemplate\n","\n","# Build prompt\n","template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n","{context}\n","Question: {question}\n","Helpful Answer:\"\"\"\n","QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n","\n","# Run chain\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    retriever=vectordb.as_retriever(),\n","    return_source_documents=True,\n","    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",")\n","\n","result = qa_chain({\"query\": question})\n","print(f\"Question: {question}\\nAnswer: {result['result']}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":2}
