{"cells":[{"cell_type":"markdown","metadata":{},"source":["# RAG Demo\n","Using RAG to power up an LLM. We will use Langchain for our example. Langchain framework makes build LLM apps super easy.\n","\n","![./flow.png](./flow.png)"]},{"cell_type":"markdown","metadata":{},"source":["## Install Packages"]},{"cell_type":"code","execution_count":1,"metadata":{"vscode":{"languageId":"shellscript"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install -q langchain chromadb pypdf langchain-openai \\\n","        tiktoken langchain-openai langchain-chroma \\\n","        sentence_transformers langchain-community \\\n","        langchain-huggingface"]},{"cell_type":"markdown","metadata":{},"source":["## Steps\n","### Step 1\n","Load a document and extract the contents. For our example, I added a sample PDF from my article in docs folder."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total pages:  4\n","First 100 chars of 2nd page:  dataset. Each model fits for few use cases.Autoenc\n","Metadata:  {'source': 'docs/GenAI-Part1.pdf', 'page': 1}\n"]}],"source":["from langchain.document_loaders import PyPDFLoader\n","loader = PyPDFLoader(\"docs/GenAI-Part1.pdf\")\n","pages = loader.load()\n","\n","# Look into the doc\n","second_page = pages[1]\n","print(\"Total pages: \", len(pages))\n","print(\"First 100 chars of 2nd page: \", second_page.page_content[:50])\n","print(\"Metadata: \", second_page.metadata)"]},{"cell_type":"markdown","metadata":{},"source":["### Step 2\n","Now split the document contents into smaller chunks."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total documents on Charter Splitter:  4\n","Total documents on Recursive Charter Splitter:  6\n","Total documents on Token Splitter:  7\n"]}],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, TokenTextSplitter\n","\n","# Character Splitter\n","c_splitter = CharacterTextSplitter(\n","    chunk_size=1000,\n","    chunk_overlap=150,\n","    separator = '\\n',\n","    length_function = len\n",")\n","\n","c_docs = c_splitter.split_documents(pages)\n","print(\"Total documents on Charter Splitter: \", len(c_docs))\n","\n","# Recursive Character Splitter\n","r_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,\n","    chunk_overlap=150, \n","    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",")\n","\n","r_docs = r_splitter.split_documents(pages)\n","print(\"Total documents on Recursive Charter Splitter: \", len(r_docs))\n","\n","# Token Splitter\n","t_splitter = TokenTextSplitter(\n","    chunk_size=150,\n","    chunk_overlap=10\n",")\n","\n","t_docs = t_splitter.split_documents(pages)\n","print(\"Total documents on Token Splitter: \", len(t_docs))\n"]},{"cell_type":"markdown","metadata":{},"source":["### Step 3\n","Let's take our splits and embed them and then store them into a vector store. We will use [ChromaDB](https://python.langchain.com/docs/integrations/vectorstores/chroma) which is an in-memory DB."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from langchain_chroma import Chroma\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_community.embeddings.sentence_transformer import (\n","    SentenceTransformerEmbeddings,\n",")\n","\n","embedding = OpenAIEmbeddings()\n","persist_directory = 'docs/chroma/'"]},{"cell_type":"code","execution_count":6,"metadata":{"vscode":{"languageId":"shellscript"}},"outputs":[],"source":["!rm -rf ./docs/chroma  # remove old database files if any\n","!mkdir -p ./docs/chroma  # create a directory to store the database files"]},{"cell_type":"markdown","metadata":{},"source":["Let's store the pages of our PDF into Vector Store with Embeddings using OpenAI Embeddings."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total documents on Vector Store:  28\n"]}],"source":["from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n","\n","# create the open-source embedding function\n","embedding_function = SentenceTransformerEmbeddings(\n","    model_name=\"all-MiniLM-L6-v2\")\n","\n","vectordb = Chroma.from_documents(pages, embedding_function)\n","\n","print(\"Total documents on Vector Store: \", vectordb._collection.count())"]},{"cell_type":"markdown","metadata":{},"source":["We will do some search and see some relevant content in this vector db."]},{"cell_type":"markdown","metadata":{},"source":["### Step 4\n","Let's retrieve with different methods"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Similarity Search\n","Page #2  : The measurement used to understand the training time is 1 “petaflop/s-day”= # floating point operati\n","Page #2  : The measurement used to understand the training time is 1 “petaflop/s-day”= # floating point operati\n","Page #2  : The measurement used to understand the training time is 1 “petaflop/s-day”= # floating point operati\n","Page #2  : The measurement used to understand the training time is 1 “petaflop/s-day”= # floating point operati\n","\n","MMR Search\n","Page #2  : The measurement used to understand the training time is 1 “petaflop/s-day”= # floating point operati\n","Page #2  : The measurement used to understand the training time is 1 “petaflop/s-day”= # floating point operati\n"]}],"source":["question = \"What is DDP?\"\n","\n","# Using Similarity Search\n","print(\"\\nSimilarity Search\")\n","docs = vectordb.similarity_search_with_score(question)\n","for d in docs:\n","    print(f\"Page #{d[0].metadata['page']}  : {d[0].page_content[:100]}\")\n","\n","# Using MMR to diversify the results\n","print(\"\\nMMR Search\")\n","docs = vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)\n","for d in docs:\n","    print(f\"Page #{d.metadata['page']}  : {d.page_content[:100]}\")"]},{"cell_type":"markdown","metadata":{},"source":["We need to do some compression to avoid unnecessary text around the content we are looking for."]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Document 1:\n","\n","Generative AI project — Part 1Muthu ArumugamThis article will help you understand how you can get involved and createproducts using Generative AI models. To get a quick intro to Generative AI,look at my previous articles — Quickies.For an AI project, these are the following steps equivalent to SDLC.\n","AI project lifecycleUse case discoveryYou have the option to choose from a variety of tasks from LLMs. You canchoose 1 or many for your project from below:Essay WritingSummarizationTranslation from language to languageTranslation from language to codeInformation retrievalCall external APIsModel identificationThe existing model may be sufficient or you have to pre-train with your\n","----------------------------------------------------------------------------------------------------\n","Document 2:\n","\n","- Bloomberg published a model which is trained for finance-related LLM.\n","- Introducing BloombergGPT, Bloomberg’s 50-billion parameter large language model, purpose-built from scratch for finance.\n","- Hugging Face Model hub — https://huggingface.co/models\n","- Hugging Face Tasks — https://huggingface.co/tasks\n","- We now can see how to choose a model and think about computing resources for training purposes.\n","- Find the one that fits closely what you need rather than plan on training it if possible.\n","- The title image was generated by DALL-E through the Bing chatbot.\n"]}],"source":["from langchain_openai import OpenAI\n","from langchain.retrievers import ContextualCompressionRetriever\n","from langchain.retrievers.document_compressors import LLMChainExtractor\n","\n","def pretty_print_docs(docs):\n","    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n","\n","# Wrap our vectorstore\n","llm = OpenAI(temperature=0)\n","compressor = LLMChainExtractor.from_llm(llm)\n","\n","compression_retriever = ContextualCompressionRetriever(\n","    base_compressor=compressor,\n","    base_retriever=vectordb.as_retriever(search_type=\"mmr\")\n",")\n","\n","compressed_docs = compression_retriever.get_relevant_documents(question)\n","pretty_print_docs(compressed_docs)"]},{"cell_type":"markdown","metadata":{},"source":["### Step 5\n","Time to call ChatGPT for a response based on our retrieval. We will use Question & Answer to call LLM.\n","\n","#### Using Retrieval QA Chain"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/muthuka/.pyenv/versions/3.12.5/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n","  warn_deprecated(\n","/Users/muthuka/.pyenv/versions/3.12.5/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n","  warn_deprecated(\n"]},{"name":"stdout","output_type":"stream","text":["Question: What is DDP?\n","Answer: I don't have enough information to accurately answer your question about DDP.\n"]}],"source":["from langchain.chat_models import ChatOpenAI\n","from langchain.chains import RetrievalQA\n","\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    retriever=vectordb.as_retriever()\n",")\n","\n","result = qa_chain({\"query\": question})\n","print(f\"Question: {question}\\nAnswer: {result['result']}\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Using Prompt"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Question: What is DDP?\n","Answer: I'm sorry, I don't have enough information to answer that question. Thanks for asking!\n"]}],"source":["from langchain.prompts import PromptTemplate\n","\n","# Build prompt\n","template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n","{context}\n","Question: {question}\n","Helpful Answer:\"\"\"\n","QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n","\n","# Run chain\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    retriever=vectordb.as_retriever(),\n","    return_source_documents=True,\n","    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",")\n","\n","result = qa_chain({\"query\": question})\n","print(f\"Question: {question}\\nAnswer: {result['result']}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":2}
